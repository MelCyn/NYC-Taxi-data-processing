{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "For this project, **the NYC Taxi Trip dataset** (specifically, **the Yellow Taxi Trip dataset**) was used because of its suitability for large-scale data processing, it being publicly available and fit for this project."
      ],
      "metadata": {
        "id": "KQ2dZ_l15nw8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we start by mounting the Google Drive and save in a path"
      ],
      "metadata": {
        "id": "L6iDWz-85w2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define folder path\n",
        "data_path = '/content/drive/MyDrive/NYC_Taxi_Trip'\n",
        "\n",
        "# Clean up the folder if it already exists, and recreate it\n",
        "if os.path.exists(data_path):\n",
        "    shutil.rmtree(data_path)\n",
        "os.makedirs(data_path, exist_ok=True)\n",
        "\n",
        "print(f\"Environment setup complete. Data will be saved in: {data_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WNcSS_863Lz",
        "outputId": "6e741684-e22d-4f7d-f080-7e10fafc08e6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Environment setup complete. Data will be saved in: /content/drive/MyDrive/NYC_Taxi_Trip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download the files**\n",
        "\n",
        "To meet the 2+ GB requirement, multiple months were downloaded  (i.e., Jan, Feb, Mar, Apr of 2024) and concatenated into a single DataFrame."
      ],
      "metadata": {
        "id": "Zy197GpyM7GL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files = {\n",
        "    \"yellow_tripdata_2024-01.parquet\": \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\",\n",
        "    \"yellow_tripdata_2024-02.parquet\": \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-02.parquet\",\n",
        "    \"yellow_tripdata_2024-03.parquet\": \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-03.parquet\",\n",
        "    \"yellow_tripdata_2024-04.parquet\": \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-04.parquet\"\n",
        "}\n",
        "\n",
        "# Download only if not already present\n",
        "for file_name, url in files.items():\n",
        "    file_path = os.path.join(data_path, file_name)\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"Downloading {file_name}...\")\n",
        "        !wget -P {data_path} {url}\n",
        "    else:\n",
        "        print(f\"{file_name} already exists. Skipping download.\")\n",
        "\n",
        "print(\"\\nAll files are downloaded and verified.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovlY0OmO63dg",
        "outputId": "56e45564-e7ed-4753-eefb-6742693ae4cd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading yellow_tripdata_2024-01.parquet...\n",
            "--2025-05-20 22:11:16--  https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\n",
            "Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 65.8.245.50, 65.8.245.51, 65.8.245.171, ...\n",
            "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|65.8.245.50|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49961641 (48M) [binary/octet-stream]\n",
            "Saving to: ‘/content/drive/MyDrive/NYC_Taxi_Trip/yellow_tripdata_2024-01.parquet’\n",
            "\n",
            "yellow_tripdata_202 100%[===================>]  47.65M  33.1MB/s    in 1.4s    \n",
            "\n",
            "2025-05-20 22:11:17 (33.1 MB/s) - ‘/content/drive/MyDrive/NYC_Taxi_Trip/yellow_tripdata_2024-01.parquet’ saved [49961641/49961641]\n",
            "\n",
            "Downloading yellow_tripdata_2024-02.parquet...\n",
            "--2025-05-20 22:11:17--  https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-02.parquet\n",
            "Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 65.8.245.50, 65.8.245.51, 65.8.245.171, ...\n",
            "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|65.8.245.50|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 50349284 (48M) [binary/octet-stream]\n",
            "Saving to: ‘/content/drive/MyDrive/NYC_Taxi_Trip/yellow_tripdata_2024-02.parquet’\n",
            "\n",
            "yellow_tripdata_202 100%[===================>]  48.02M  34.9MB/s    in 1.4s    \n",
            "\n",
            "2025-05-20 22:11:19 (34.9 MB/s) - ‘/content/drive/MyDrive/NYC_Taxi_Trip/yellow_tripdata_2024-02.parquet’ saved [50349284/50349284]\n",
            "\n",
            "Downloading yellow_tripdata_2024-03.parquet...\n",
            "--2025-05-20 22:11:19--  https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-03.parquet\n",
            "Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 65.8.245.50, 65.8.245.51, 65.8.245.171, ...\n",
            "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|65.8.245.50|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 60078280 (57M) [binary/octet-stream]\n",
            "Saving to: ‘/content/drive/MyDrive/NYC_Taxi_Trip/yellow_tripdata_2024-03.parquet’\n",
            "\n",
            "yellow_tripdata_202 100%[===================>]  57.29M  29.0MB/s    in 2.0s    \n",
            "\n",
            "2025-05-20 22:11:21 (29.0 MB/s) - ‘/content/drive/MyDrive/NYC_Taxi_Trip/yellow_tripdata_2024-03.parquet’ saved [60078280/60078280]\n",
            "\n",
            "Downloading yellow_tripdata_2024-04.parquet...\n",
            "--2025-05-20 22:11:21--  https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-04.parquet\n",
            "Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 65.8.245.50, 65.8.245.51, 65.8.245.171, ...\n",
            "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|65.8.245.50|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 59133625 (56M) [binary/octet-stream]\n",
            "Saving to: ‘/content/drive/MyDrive/NYC_Taxi_Trip/yellow_tripdata_2024-04.parquet’\n",
            "\n",
            "yellow_tripdata_202 100%[===================>]  56.39M  43.4MB/s    in 1.3s    \n",
            "\n",
            "2025-05-20 22:11:22 (43.4 MB/s) - ‘/content/drive/MyDrive/NYC_Taxi_Trip/yellow_tripdata_2024-04.parquet’ saved [59133625/59133625]\n",
            "\n",
            "\n",
            "All files are downloaded and verified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking if the paths are downloaded and formatted properly**"
      ],
      "metadata": {
        "id": "Bk2owySa6Mwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Files in the folder:\")\n",
        "print(os.listdir(data_path))\n"
      ],
      "metadata": {
        "id": "swCTX_l963hy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a6258a0-7e08-434c-9493-01ad6e109c95"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in the folder:\n",
            "['yellow_tripdata_2024-01.parquet', 'yellow_tripdata_2024-02.parquet', 'yellow_tripdata_2024-03.parquet', 'yellow_tripdata_2024-04.parquet']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "****"
      ],
      "metadata": {
        "id": "EDBLlVHI6qsu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The code below was done to prevent Google Colab from timing out and having to start afresh**"
      ],
      "metadata": {
        "id": "VfP9HpXyN7jt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import time\n",
        "import IPython\n",
        "\n",
        "# Function to keep the session alive\n",
        "def prevent_timeout():\n",
        "    while True:\n",
        "        time.sleep(60)\n",
        "        IPython.display.display(IPython.display.Javascript('console.log(\"Preventing Timeout...\");'))\n",
        "\n",
        "# Start the thread\n",
        "thread = threading.Thread(target=prevent_timeout)\n",
        "thread.daemon = True\n",
        "thread.start()\n",
        "\n"
      ],
      "metadata": {
        "id": "7ZMRU7-563nJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The next step is to read the file**.\n",
        "\n",
        "We'll try different methods for reading these parquet files and compare their computational efficiency. The methods we'll explore are:\n",
        "\n",
        "1) Pandas\n",
        "\n",
        "2) Dask\n",
        "\n",
        "**NB: Modin was explored but resulted to having errors due to RAM not being enough for the execution. So, the decision was made to make use of just Pandas and Dask.**"
      ],
      "metadata": {
        "id": "TifPBdb767fY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1**) **PANDAS**"
      ],
      "metadata": {
        "id": "hbesM4yl_y0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Define the folder path and file names\n",
        "data_path = '/content/drive/MyDrive/NYC_Taxi_Trip'\n",
        "file_paths = [\n",
        "    f\"{data_path}/yellow_tripdata_2024-01.parquet\",\n",
        "    f\"{data_path}/yellow_tripdata_2024-02.parquet\",\n",
        "    f\"{data_path}/yellow_tripdata_2024-03.parquet\",\n",
        "    f\"{data_path}/yellow_tripdata_2024-04.parquet\"\n",
        "]\n",
        "\n",
        "# Start timer\n",
        "start_time = time.time()\n",
        "\n",
        "# Read all four files into DataFrames\n",
        "pandas_dfs = [pd.read_parquet(file) for file in file_paths]\n",
        "\n",
        "# Concatenate them into a single DataFrame\n",
        "pandas_full_data = pd.concat(pandas_dfs, ignore_index=True)\n",
        "\n",
        "# End timer\n",
        "end_time = time.time()\n",
        "\n",
        "# Display the time taken and DataFrame info\n",
        "print(f\"Time taken to read with Pandas: {end_time - start_time:.2f} seconds\")\n",
        "print(f\"DataFrame shape: {pandas_full_data.shape}\")\n",
        "print(\"\\n--- Sample Data ---\")\n",
        "print(pandas_full_data.head())\n"
      ],
      "metadata": {
        "id": "cWzEWS_o63ps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5d4828c-e373-495c-95dd-0b935262f696"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken to read with Pandas: 6.01 seconds\n",
            "DataFrame shape: (13069067, 19)\n",
            "\n",
            "--- Sample Data ---\n",
            "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
            "0         2  2024-01-01 00:57:55   2024-01-01 01:17:43              1.0   \n",
            "1         1  2024-01-01 00:03:00   2024-01-01 00:09:36              1.0   \n",
            "2         1  2024-01-01 00:17:06   2024-01-01 00:35:01              1.0   \n",
            "3         1  2024-01-01 00:36:38   2024-01-01 00:44:56              1.0   \n",
            "4         1  2024-01-01 00:46:51   2024-01-01 00:52:57              1.0   \n",
            "\n",
            "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
            "0           1.72         1.0                  N           186            79   \n",
            "1           1.80         1.0                  N           140           236   \n",
            "2           4.70         1.0                  N           236            79   \n",
            "3           1.40         1.0                  N            79           211   \n",
            "4           0.80         1.0                  N           211           148   \n",
            "\n",
            "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
            "0             2         17.7    1.0      0.5        0.00           0.0   \n",
            "1             1         10.0    3.5      0.5        3.75           0.0   \n",
            "2             1         23.3    3.5      0.5        3.00           0.0   \n",
            "3             1         10.0    3.5      0.5        2.00           0.0   \n",
            "4             1          7.9    3.5      0.5        3.20           0.0   \n",
            "\n",
            "   improvement_surcharge  total_amount  congestion_surcharge  Airport_fee  \n",
            "0                    1.0         22.70                   2.5          0.0  \n",
            "1                    1.0         18.75                   2.5          0.0  \n",
            "2                    1.0         31.30                   2.5          0.0  \n",
            "3                    1.0         17.00                   2.5          0.0  \n",
            "4                    1.0         16.10                   2.5          0.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pandas Approach (pd.read_parquet)**:\n",
        "\n",
        "This is the traditional way of reading parquet files in Python.\n",
        "It loads the entire data into memory, which is fast for small datasets but not memory-efficient for large datasets.\n",
        "\n",
        "We measure the time it takes to load and check the shape of the DataFrame.\n",
        "\n",
        "**What This Code Does**:\n",
        "\n",
        "- Lists the file paths for each parquet file we downloaded.\n",
        "\n",
        "- Reads each file into separate DataFrames.\n",
        "\n",
        "- Concatenates them into one large DataFrame.\n",
        "\n",
        "- Measures the time taken to read and concatenate.\n",
        "\n",
        "- Displays the first few rows for inspection.\n",
        "\n",
        "**The expected output: The Analysis** shows;\n",
        "\n",
        "1) Time Taken: 6.01 seconds is pretty decent for 13 million rows.\n",
        "\n",
        "2) DataFrame Shape: 13,069,067 rows × 19 columns which is expected for combining four large parquet files.\n",
        "\n",
        "3) Data Sample: Data looks clean and well-structured with clear column names."
      ],
      "metadata": {
        "id": "Yk2I5jtB8S37"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2**)  **DASK**"
      ],
      "metadata": {
        "id": "aMME8e4s_mbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dask.dataframe as dd\n",
        "import time\n",
        "\n",
        "# Start timer\n",
        "start_time = time.time()\n",
        "\n",
        "# Read all Parquet files using Dask\n",
        "dask_df = dd.read_parquet(f\"{data_path}/*.parquet\")\n",
        "\n",
        "# Compute the DataFrame to load it fully into memory and measure the time\n",
        "dask_full_data = dask_df.compute()\n",
        "\n",
        "# End timer\n",
        "end_time = time.time()\n",
        "\n",
        "# Display the time taken and DataFrame info\n",
        "print(f\"Time taken to read with Dask: {end_time - start_time:.2f} seconds\")\n",
        "print(f\"DataFrame shape: {dask_full_data.shape}\")\n",
        "print(\"\\n--- Sample Data ---\")\n",
        "print(dask_full_data.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "si9UAGyJ63sC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41ed3623-dc01-44fa-98e5-f110e42628d1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken to read with Dask: 10.94 seconds\n",
            "DataFrame shape: (13069067, 19)\n",
            "\n",
            "--- Sample Data ---\n",
            "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
            "0         2  2024-01-01 00:57:55   2024-01-01 01:17:43              1.0   \n",
            "1         1  2024-01-01 00:03:00   2024-01-01 00:09:36              1.0   \n",
            "2         1  2024-01-01 00:17:06   2024-01-01 00:35:01              1.0   \n",
            "3         1  2024-01-01 00:36:38   2024-01-01 00:44:56              1.0   \n",
            "4         1  2024-01-01 00:46:51   2024-01-01 00:52:57              1.0   \n",
            "\n",
            "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
            "0           1.72         1.0                  N           186            79   \n",
            "1           1.80         1.0                  N           140           236   \n",
            "2           4.70         1.0                  N           236            79   \n",
            "3           1.40         1.0                  N            79           211   \n",
            "4           0.80         1.0                  N           211           148   \n",
            "\n",
            "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
            "0             2         17.7    1.0      0.5        0.00           0.0   \n",
            "1             1         10.0    3.5      0.5        3.75           0.0   \n",
            "2             1         23.3    3.5      0.5        3.00           0.0   \n",
            "3             1         10.0    3.5      0.5        2.00           0.0   \n",
            "4             1          7.9    3.5      0.5        3.20           0.0   \n",
            "\n",
            "   improvement_surcharge  total_amount  congestion_surcharge  Airport_fee  \n",
            "0                    1.0         22.70                   2.5          0.0  \n",
            "1                    1.0         18.75                   2.5          0.0  \n",
            "2                    1.0         31.30                   2.5          0.0  \n",
            "3                    1.0         17.00                   2.5          0.0  \n",
            "4                    1.0         16.10                   2.5          0.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dask Approach (dd.read_parquet)**:\n",
        "\n",
        "Dask is designed to handle large datasets that don't fit into memory.\n",
        "It reads everything at once, loads data in chunks and processes them in parallel.\n",
        "\n",
        "The DataFrame created is a \"lazy\" one, which means it doesn't actually load all the data until you perform an operation like .compute().\n",
        "\n",
        "It is said to be much more scalable than Pandas for large datasets.\n",
        "\n",
        "**Analysis**:\n",
        "\n",
        "- Time Taken: 10.94 seconds with Dask compared to 6.01 seconds with Pandas.\n",
        "\n",
        "- DataFrame Shape: Identical to the Pandas read — 13,069,067 rows × 19 columns.\n",
        "\n",
        "- Data Sample: Data looks clean and matches the structure from the Pandas read.\n",
        "\n",
        "**Observations**:\n",
        "\n",
        "Dask is slower here than Pandas which is surprising but maybe not entirely unexpected for these reasons:\n",
        "\n",
        "- Dask shines when the dataset does not fit into memory.\n",
        "\n",
        "- the dataset was small enough (500 MB combined) for Pandas to load it faster since it works in-memory.\n",
        "\n",
        "- Dask is optimized for parallelism and out-of-memory computation, but in this case, Pandas did not face memory constraints.\n",
        "\n",
        "**Probably, Dask would have done better**\n",
        "\n",
        "If this was a 5GB or 10GB dataset, it would have most likely outperform Pandas.\n",
        "\n",
        "If we were performing complex operations (like groupby, joins, or merges) on large chunks, Dask's chunk-based processing would be faster."
      ],
      "metadata": {
        "id": "_9xaKEFJB5m8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DATA VALIDATION and CLEANING**\n",
        "\n",
        "- Checking for missing values"
      ],
      "metadata": {
        "id": "etQjdWTmVyWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Check for Missing Values\n",
        "print(\"🔍 Checking for Missing Values...\")\n",
        "\n",
        "# Calculate the number and percentage of missing values\n",
        "missing_values = pandas_full_data.isna().sum()\n",
        "missing_percentage = (missing_values / len(pandas_full_data)) * 100\n",
        "\n",
        "# Combine into a DataFrame for better readability\n",
        "missing_data = pd.DataFrame({\n",
        "    'Missing Values': missing_values,\n",
        "    'Percentage (%)': missing_percentage\n",
        "}).sort_values(by='Missing Values', ascending=False)\n",
        "\n",
        "# Display columns with missing values only\n",
        "missing_data = missing_data[missing_data['Missing Values'] > 0]\n",
        "\n",
        "# Output the result\n",
        "if missing_data.empty:\n",
        "    print(\"✅ No missing values found!\")\n",
        "else:\n",
        "    print(\"⚠️ Missing values detected:\")\n",
        "    print(missing_data)\n"
      ],
      "metadata": {
        "id": "eJ9S0KLC63ub",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a38b3ae8-07f7-479b-f81e-60b4d3ccc3a4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Checking for Missing Values...\n",
            "⚠️ Missing values detected:\n",
            "                      Missing Values  Percentage (%)\n",
            "store_and_fwd_flag           1160538        8.880037\n",
            "RatecodeID                   1160538        8.880037\n",
            "passenger_count              1160538        8.880037\n",
            "Airport_fee                  1160538        8.880037\n",
            "congestion_surcharge         1160538        8.880037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the columns with missing values have the same number of NaNs (8.88% of the dataset). It may be that these rows might be related to a specific set of trips that might not have been recorded.\n",
        "\n",
        "Further investigation into the dataset will give a clearer view on what could be done with the missing values."
      ],
      "metadata": {
        "id": "_HgNtWZyYBu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter rows with missing values\n",
        "missing_data_rows = pandas_full_data[pandas_full_data.isna().any(axis=1)]\n",
        "\n",
        "# Display basic info\n",
        "print(\"Number of rows with missing values:\", len(missing_data_rows))\n",
        "print(\"\\n--- Sample of rows with missing values ---\")\n",
        "display(missing_data_rows.head())\n",
        "\n",
        "# Check distribution by month\n",
        "print(\"\\n🔍 Distribution by Month:\")\n",
        "missing_data_rows['tpep_pickup_datetime'] = pd.to_datetime(missing_data_rows['tpep_pickup_datetime'])\n",
        "print(missing_data_rows['tpep_pickup_datetime'].dt.month.value_counts())\n",
        "\n",
        "# Check distribution by VendorID\n",
        "print(\"\\n🔍 Distribution by VendorID:\")\n",
        "print(missing_data_rows['VendorID'].value_counts())\n",
        "\n",
        "# Check distribution by PULocationID (pickup location)\n",
        "print(\"\\n🔍 Top 10 Pickup Locations with Missing Data:\")\n",
        "print(missing_data_rows['PULocationID'].value_counts().head(10))\n",
        "\n",
        "# Check if all columns are missing at the same time\n",
        "print(\"\\n🔍 Are all these columns missing simultaneously?\")\n",
        "print(missing_data_rows[['store_and_fwd_flag', 'RatecodeID', 'passenger_count', 'Airport_fee', 'congestion_surcharge']].isna().all(axis=1).value_counts())\n"
      ],
      "metadata": {
        "id": "H1Dtt0l563yz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        },
        "outputId": "27586c2a-c432-4039-f2e6-5af244979420"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows with missing values: 1160538\n",
            "\n",
            "--- Sample of rows with missing values ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
              "2824462         2  2024-01-01 00:34:19   2024-01-01 00:51:22              NaN   \n",
              "2824463         1  2024-01-01 00:14:31   2024-01-01 00:19:29              NaN   \n",
              "2824464         1  2024-01-01 00:35:11   2024-01-01 01:13:40              NaN   \n",
              "2824465         1  2024-01-01 00:33:37   2024-01-01 00:50:34              NaN   \n",
              "2824466         1  2024-01-01 00:49:04   2024-01-01 01:01:16              NaN   \n",
              "\n",
              "         trip_distance  RatecodeID store_and_fwd_flag  PULocationID  \\\n",
              "2824462           2.04         NaN               None           143   \n",
              "2824463           1.60         NaN               None           236   \n",
              "2824464           0.00         NaN               None           142   \n",
              "2824465           0.00         NaN               None           237   \n",
              "2824466           0.00         NaN               None           244   \n",
              "\n",
              "         DOLocationID  payment_type  fare_amount  extra  mta_tax  tip_amount  \\\n",
              "2824462           141             0        12.72    0.0      0.5        0.00   \n",
              "2824463           238             0         9.30    1.0      0.5        2.86   \n",
              "2824464            79             0        21.01    0.0      0.5        0.00   \n",
              "2824465             4             0        17.79    0.0      0.5        0.00   \n",
              "2824466            50             0        34.65    0.0      0.5        0.00   \n",
              "\n",
              "         tolls_amount  improvement_surcharge  total_amount  \\\n",
              "2824462           0.0                    1.0         16.72   \n",
              "2824463           0.0                    1.0         17.16   \n",
              "2824464           0.0                    1.0         25.01   \n",
              "2824465           0.0                    1.0         21.79   \n",
              "2824466           0.0                    1.0         38.65   \n",
              "\n",
              "         congestion_surcharge  Airport_fee  \n",
              "2824462                   NaN          NaN  \n",
              "2824463                   NaN          NaN  \n",
              "2824464                   NaN          NaN  \n",
              "2824465                   NaN          NaN  \n",
              "2824466                   NaN          NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e1eb258a-e8ec-4773-8a41-bd512cd9c35d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>VendorID</th>\n",
              "      <th>tpep_pickup_datetime</th>\n",
              "      <th>tpep_dropoff_datetime</th>\n",
              "      <th>passenger_count</th>\n",
              "      <th>trip_distance</th>\n",
              "      <th>RatecodeID</th>\n",
              "      <th>store_and_fwd_flag</th>\n",
              "      <th>PULocationID</th>\n",
              "      <th>DOLocationID</th>\n",
              "      <th>payment_type</th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>extra</th>\n",
              "      <th>mta_tax</th>\n",
              "      <th>tip_amount</th>\n",
              "      <th>tolls_amount</th>\n",
              "      <th>improvement_surcharge</th>\n",
              "      <th>total_amount</th>\n",
              "      <th>congestion_surcharge</th>\n",
              "      <th>Airport_fee</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2824462</th>\n",
              "      <td>2</td>\n",
              "      <td>2024-01-01 00:34:19</td>\n",
              "      <td>2024-01-01 00:51:22</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.04</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>143</td>\n",
              "      <td>141</td>\n",
              "      <td>0</td>\n",
              "      <td>12.72</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>16.72</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2824463</th>\n",
              "      <td>1</td>\n",
              "      <td>2024-01-01 00:14:31</td>\n",
              "      <td>2024-01-01 00:19:29</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.60</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>236</td>\n",
              "      <td>238</td>\n",
              "      <td>0</td>\n",
              "      <td>9.30</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2.86</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>17.16</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2824464</th>\n",
              "      <td>1</td>\n",
              "      <td>2024-01-01 00:35:11</td>\n",
              "      <td>2024-01-01 01:13:40</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>142</td>\n",
              "      <td>79</td>\n",
              "      <td>0</td>\n",
              "      <td>21.01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>25.01</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2824465</th>\n",
              "      <td>1</td>\n",
              "      <td>2024-01-01 00:33:37</td>\n",
              "      <td>2024-01-01 00:50:34</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>237</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>17.79</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>21.79</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2824466</th>\n",
              "      <td>1</td>\n",
              "      <td>2024-01-01 00:49:04</td>\n",
              "      <td>2024-01-01 01:01:16</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>244</td>\n",
              "      <td>50</td>\n",
              "      <td>0</td>\n",
              "      <td>34.65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>38.65</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1eb258a-e8ec-4773-8a41-bd512cd9c35d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e1eb258a-e8ec-4773-8a41-bd512cd9c35d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e1eb258a-e8ec-4773-8a41-bd512cd9c35d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-5da8c172-f244-4bac-9035-7ea4c9148a9a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5da8c172-f244-4bac-9035-7ea4c9148a9a')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-5da8c172-f244-4bac-9035-7ea4c9148a9a button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍 Distribution by Month:\n",
            "tpep_pickup_datetime\n",
            "3    426190\n",
            "4    408576\n",
            "2    185610\n",
            "1    140162\n",
            "Name: count, dtype: int64\n",
            "\n",
            "🔍 Distribution by VendorID:\n",
            "VendorID\n",
            "2    857921\n",
            "1    301604\n",
            "6      1013\n",
            "Name: count, dtype: int64\n",
            "\n",
            "🔍 Top 10 Pickup Locations with Missing Data:\n",
            "PULocationID\n",
            "79     49534\n",
            "236    37802\n",
            "249    35310\n",
            "239    35041\n",
            "161    33311\n",
            "234    30155\n",
            "68     27512\n",
            "170    27408\n",
            "107    27300\n",
            "262    27298\n",
            "Name: count, dtype: int64\n",
            "\n",
            "🔍 Are all these columns missing simultaneously?\n",
            "True    1160538\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-c52c1f458825>:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  missing_data_rows['tpep_pickup_datetime'] = pd.to_datetime(missing_data_rows['tpep_pickup_datetime'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the output above, it will be better to clean by removing/dropping the rows with missing values."
      ],
      "metadata": {
        "id": "yDWt0J4JbLTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 🧹 Step 1: Remove rows where all five columns are missing\n",
        "columns_to_check = [\n",
        "    'passenger_count',\n",
        "    'RatecodeID',\n",
        "    'store_and_fwd_flag',\n",
        "    'congestion_surcharge',\n",
        "    'Airport_fee'\n",
        "]\n",
        "\n",
        "# Filtering out rows where all specified columns are NaN\n",
        "cleaned_data = dask_full_data.dropna(subset=columns_to_check)\n",
        "\n",
        "# 📝 Step 2: Validation\n",
        "print(f\"Original DataFrame Shape: {dask_full_data.shape}\")\n",
        "print(f\"New DataFrame Shape after Cleanup: {cleaned_data.shape}\")\n",
        "\n",
        "# Checking for any remaining missing values\n",
        "missing_summary = cleaned_data.isna().sum()\n",
        "print(\"\\n🔍 Remaining Missing Values after Cleanup:\")\n",
        "print(missing_summary[missing_summary > 0])\n",
        "\n",
        "# Checking the distribution by month and VendorID\n",
        "print(\"\\n🔍 Distribution by Month after Cleanup:\")\n",
        "print(cleaned_data['tpep_pickup_datetime'].dt.month.value_counts())\n",
        "\n",
        "print(\"\\n🔍 Distribution by VendorID after Cleanup:\")\n",
        "print(cleaned_data['VendorID'].value_counts())\n"
      ],
      "metadata": {
        "id": "AUIm2ipe634V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27861797-1702-4299-d7da-7cc644fe1bef"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame Shape: (13069067, 19)\n",
            "New DataFrame Shape after Cleanup: (11908529, 19)\n",
            "\n",
            "🔍 Remaining Missing Values after Cleanup:\n",
            "Series([], dtype: int64)\n",
            "\n",
            "🔍 Distribution by Month after Cleanup:\n",
            "tpep_pickup_datetime\n",
            "3     3156421\n",
            "4     3105706\n",
            "1     2824459\n",
            "2     2821923\n",
            "12         19\n",
            "5           1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "🔍 Distribution by VendorID after Cleanup:\n",
            "VendorID\n",
            "2    9024022\n",
            "1    2884507\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next is to validate the data and handle all anormalities"
      ],
      "metadata": {
        "id": "rpHqXZ0lespH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Investigate the anomalies for May and December\n",
        "anomalies = cleaned_data[cleaned_data['tpep_pickup_datetime'].dt.month.isin([5, 12])]\n",
        "print(\"\\n🔎 Anomalous Records (May and December):\")\n",
        "print(anomalies)\n",
        "\n",
        "# Step 2: Data Type Validation\n",
        "print(\"\\n🔎 Data Types:\")\n",
        "print(cleaned_data.dtypes)\n",
        "\n",
        "# Step 3: Range Checks\n",
        "print(\"\\n🔎 Range Check Summaries:\")\n",
        "print(f\"Passenger Count: {cleaned_data['passenger_count'].min()} to {cleaned_data['passenger_count'].max()}\")\n",
        "print(f\"Fare Amount: {cleaned_data['fare_amount'].min()} to {cleaned_data['fare_amount'].max()}\")\n",
        "print(f\"Trip Distance: {cleaned_data['trip_distance'].min()} to {cleaned_data['trip_distance'].max()}\")\n"
      ],
      "metadata": {
        "id": "_ooPzcy0637U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5017fe16-15dc-4ff8-9e83-f45b4a56e1ce"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔎 Anomalous Records (May and December):\n",
            "         VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
            "256             2  2023-12-31 23:56:46   2024-01-01 00:12:06              2.0   \n",
            "369             2  2023-12-31 23:39:17   2023-12-31 23:42:00              2.0   \n",
            "753             2  2023-12-31 23:41:02   2023-12-31 23:48:03              1.0   \n",
            "2210            2  2023-12-31 23:57:17   2024-01-01 00:01:50              1.0   \n",
            "2615            2  2023-12-31 23:56:45   2024-01-01 00:00:28              1.0   \n",
            "2985            2  2023-12-31 23:49:12   2024-01-01 00:04:32              1.0   \n",
            "3176            2  2023-12-31 23:47:28   2023-12-31 23:57:07              2.0   \n",
            "4137            2  2023-12-31 23:58:35   2024-01-01 00:13:06              6.0   \n",
            "4142            2  2023-12-31 23:58:37   2024-01-01 00:08:37              2.0   \n",
            "8628            2  2023-12-31 23:54:27   2024-01-01 00:13:12              1.0   \n",
            "53119           2  2002-12-31 22:59:39   2002-12-31 23:05:41              1.0   \n",
            "53120           2  2002-12-31 22:59:39   2002-12-31 23:05:41              1.0   \n",
            "181283          2  2008-12-31 22:52:49   2008-12-31 23:04:09              1.0   \n",
            "1670751         2  2002-12-31 22:17:10   2002-12-31 22:42:24              1.0   \n",
            "1811092         2  2002-12-31 23:08:30   2003-01-01 14:58:35              1.0   \n",
            "296209          2  2002-12-31 22:10:04   2002-12-31 22:14:22              1.0   \n",
            "514718          2  2002-12-31 23:09:40   2003-01-01 20:22:32              1.0   \n",
            "611171          2  2002-12-31 22:19:28   2003-01-01 18:58:09              1.0   \n",
            "1824633         2  2008-12-31 23:06:01   2009-01-01 16:10:33              1.0   \n",
            "3103849         2  2024-05-01 00:02:36   2024-05-01 00:07:24              6.0   \n",
            "\n",
            "         trip_distance  RatecodeID store_and_fwd_flag  PULocationID  \\\n",
            "256               2.38         1.0                  N           236   \n",
            "369               0.47         1.0                  N            90   \n",
            "753               0.40         1.0                  N           246   \n",
            "2210              0.53         1.0                  N           144   \n",
            "2615              0.97         1.0                  N           163   \n",
            "2985              3.14         1.0                  N           234   \n",
            "3176              1.44         1.0                  N            68   \n",
            "4137              8.39         1.0                  N           138   \n",
            "4142              0.59         1.0                  N           161   \n",
            "8628              7.70         1.0                  N           229   \n",
            "53119             0.63         1.0                  N           170   \n",
            "53120             0.63         1.0                  N           170   \n",
            "181283            1.62         1.0                  N           141   \n",
            "1670751           1.40         1.0                  N            50   \n",
            "1811092           5.34         1.0                  N           132   \n",
            "296209            7.55         1.0                  N           170   \n",
            "514718            2.12         1.0                  N           234   \n",
            "611171            0.55         1.0                  N           246   \n",
            "1824633          16.87         2.0                  N           132   \n",
            "3103849           1.01         1.0                  N           161   \n",
            "\n",
            "         DOLocationID  payment_type  fare_amount  extra  mta_tax  tip_amount  \\\n",
            "256               142             1         15.6    1.0      0.5        1.00   \n",
            "369                68             1          5.1    1.0      0.5        0.00   \n",
            "753               246             2          7.2    1.0      0.5        0.00   \n",
            "2210              211             1          5.8    1.0      0.5        2.16   \n",
            "2615              237             1          6.5    1.0      0.5        2.00   \n",
            "2985              237             1         17.0    1.0      0.5        6.60   \n",
            "3176              137             1         10.7    1.0      0.5        3.14   \n",
            "4137              217             2         33.1    6.0      0.5        0.00   \n",
            "4142              170             1         10.0    1.0      0.5        3.75   \n",
            "8628              244             1         33.1    1.0      0.5        7.62   \n",
            "53119             170             3         -6.5    0.0     -0.5        0.00   \n",
            "53120             170             3          6.5    0.0      0.5        0.00   \n",
            "181283            211             1         11.4    2.5      0.5        2.00   \n",
            "1670751           162             1         10.0    1.0      0.5        3.00   \n",
            "1811092           124             2         21.9    0.0      0.5        0.00   \n",
            "296209            231             1         33.1    1.0      0.5        7.62   \n",
            "514718            148             2         17.7    1.0      0.5        0.00   \n",
            "611171             68             2          7.2    0.0      0.5        0.00   \n",
            "1824633           161             1         70.0    0.0      0.5       13.00   \n",
            "3103849           237             1          7.9    1.0      0.5        2.58   \n",
            "\n",
            "         tolls_amount  improvement_surcharge  total_amount  \\\n",
            "256              0.00                    1.0         21.60   \n",
            "369              0.00                    1.0         10.10   \n",
            "753              0.00                    1.0         12.20   \n",
            "2210             0.00                    1.0         12.96   \n",
            "2615             0.00                    1.0         13.50   \n",
            "2985             0.00                    1.0         28.60   \n",
            "3176             0.00                    1.0         18.84   \n",
            "4137             0.00                    1.0         42.35   \n",
            "4142             0.00                    1.0         18.75   \n",
            "8628             0.00                    1.0         45.72   \n",
            "53119            0.00                   -1.0        -10.50   \n",
            "53120            0.00                    1.0         10.50   \n",
            "181283           0.00                    1.0         19.90   \n",
            "1670751          0.00                    1.0         18.00   \n",
            "1811092          0.00                    1.0         25.15   \n",
            "296209           0.00                    1.0         45.72   \n",
            "514718           0.00                    1.0         22.70   \n",
            "611171           0.00                    1.0         11.20   \n",
            "1824633          6.94                    1.0         95.69   \n",
            "3103849          0.00                    1.0         15.48   \n",
            "\n",
            "         congestion_surcharge  Airport_fee  \n",
            "256                       2.5         0.00  \n",
            "369                       2.5         0.00  \n",
            "753                       2.5         0.00  \n",
            "2210                      2.5         0.00  \n",
            "2615                      2.5         0.00  \n",
            "2985                      2.5         0.00  \n",
            "3176                      2.5         0.00  \n",
            "4137                      0.0         1.75  \n",
            "4142                      2.5         0.00  \n",
            "8628                      2.5         0.00  \n",
            "53119                    -2.5         0.00  \n",
            "53120                     2.5         0.00  \n",
            "181283                    2.5         0.00  \n",
            "1670751                   2.5         0.00  \n",
            "1811092                   0.0         1.75  \n",
            "296209                    2.5         0.00  \n",
            "514718                    2.5         0.00  \n",
            "611171                    2.5         0.00  \n",
            "1824633                   2.5         1.75  \n",
            "3103849                   2.5         0.00  \n",
            "\n",
            "🔎 Data Types:\n",
            "VendorID                           int32\n",
            "tpep_pickup_datetime      datetime64[us]\n",
            "tpep_dropoff_datetime     datetime64[us]\n",
            "passenger_count                  float64\n",
            "trip_distance                    float64\n",
            "RatecodeID                       float64\n",
            "store_and_fwd_flag       string[pyarrow]\n",
            "PULocationID                       int32\n",
            "DOLocationID                       int32\n",
            "payment_type                       int64\n",
            "fare_amount                      float64\n",
            "extra                            float64\n",
            "mta_tax                          float64\n",
            "tip_amount                       float64\n",
            "tolls_amount                     float64\n",
            "improvement_surcharge            float64\n",
            "total_amount                     float64\n",
            "congestion_surcharge             float64\n",
            "Airport_fee                      float64\n",
            "dtype: object\n",
            "\n",
            "🔎 Range Check Summaries:\n",
            "Passenger Count: 0.0 to 9.0\n",
            "Fare Amount: -999.0 to 5000.0\n",
            "Trip Distance: 0.0 to 134298.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the Output so far, it is noticed that\n",
        "May (Month 5) has just 1 record and also December (Month 12) has Only 19 records.\n",
        "\n",
        "Given how large other months are (e.g., over 3 million records for March and also for April), this clearly shows May and December data are either incomplete or corrupted. They can be safely removed to keep this analysis meaningful."
      ],
      "metadata": {
        "id": "Gml0zDOj0Fvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove records from May (5) and December (12)\n",
        "cleaned_data = cleaned_data[~cleaned_data['tpep_pickup_datetime'].dt.month.isin([5, 12])]\n",
        "\n",
        "# Confirm removal\n",
        "print(\"\\n✅ Distribution by Month after Removal:\")\n",
        "print(cleaned_data['tpep_pickup_datetime'].dt.month.value_counts().sort_index())\n"
      ],
      "metadata": {
        "id": "Iw43OCRw6394",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7197edce-1ccf-4678-e627-73741a2a3ef5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Distribution by Month after Removal:\n",
            "tpep_pickup_datetime\n",
            "1    2824459\n",
            "2    2821923\n",
            "3    3156421\n",
            "4    3105706\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next is to create YAML schema with:\n",
        "\n",
        "Column names\n",
        "\n",
        "File separator: |\n",
        "\n",
        "and validate the DataFrame against this schema."
      ],
      "metadata": {
        "id": "ks-m8DFROHCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "# Define schema content\n",
        "schema = {\n",
        "    'separator': '|',\n",
        "    'columns': list(cleaned_data.columns)\n",
        "}\n",
        "\n",
        "# Save schema to YAML file\n",
        "with open('schema.yaml', 'w') as file:\n",
        "    yaml.dump(schema, file, sort_keys=False)\n",
        "\n",
        "print(\"\\n✅ YAML schema file 'schema.yaml' created.\")\n"
      ],
      "metadata": {
        "id": "B8OZYzD464Ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f05627b-adfa-44f9-f5a6-f3eff8e900a6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ YAML schema file 'schema.yaml' created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load schema from YAML\n",
        "with open('schema.yaml', 'r') as file:\n",
        "    loaded_schema = yaml.safe_load(file)\n",
        "\n",
        "# Compare column names\n",
        "df_columns = list(cleaned_data.columns)\n",
        "schema_columns = loaded_schema['columns']\n",
        "\n",
        "print(\"\\n✅ Column Match:\", df_columns == schema_columns)\n",
        "if df_columns != schema_columns:\n",
        "    print(\"⚠️ Mismatch detected!\")\n",
        "else:\n",
        "    print(\"✅ Column names and order match the schema.\")\n"
      ],
      "metadata": {
        "id": "IkpFjnTp64Da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e446294-7f64-4ec1-c284-3ed6b66478ae"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Column Match: True\n",
            "✅ Column names and order match the schema.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Output file path\n",
        "output_file = 'cleaned_data_output.txt.gz'\n",
        "\n",
        "# Write to .gz file with pipe (|) separator\n",
        "cleaned_data.to_csv(output_file, sep='|', index=False, compression='gzip')\n",
        "\n",
        "print(f\"\\n✅ File saved as: {output_file}\")\n"
      ],
      "metadata": {
        "id": "oLn0COsu64HC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "41501d17-4884-4cd6-cf91-3b7ee8c4231d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "console.log(\"Preventing Timeout...\");"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "console.log(\"Preventing Timeout...\");"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "console.log(\"Preventing Timeout...\");"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "console.log(\"Preventing Timeout...\");"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "console.log(\"Preventing Timeout...\");"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "console.log(\"Preventing Timeout...\");"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "console.log(\"Preventing Timeout...\");"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "console.log(\"Preventing Timeout...\");"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "console.log(\"Preventing Timeout...\");"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ File saved as: cleaned_data_output.txt.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# File size in MB\n",
        "file_size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
        "\n",
        "# Summary\n",
        "print(\"\\n📊 File Summary:\")\n",
        "print(f\"Total Rows: {cleaned_data.shape[0]}\")\n",
        "print(f\"Total Columns: {cleaned_data.shape[1]}\")\n",
        "print(f\"File Size: {file_size_mb:.2f} MB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzC5xHQf7RRL",
        "outputId": "fb6167ad-bca1-4d29-f24a-236d58c02d16"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 File Summary:\n",
            "Total Rows: 11908509\n",
            "Total Columns: 19\n",
            "File Size: 206.27 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download final data file\n",
        "files.download(\"cleaned_data_output.txt.gz\")\n",
        "\n",
        "# Download YAML schema\n",
        "files.download(\"schema.yaml\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "UpZmiumt7RBY",
        "outputId": "30ebfa6b-712b-4975-eaf9-d9ab5115fb46"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f019e227-ac1d-4b53-bbd0-4b1390f1aeea\", \"cleaned_data_output.txt.gz\", 216293186)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_71ddec50-fbf2-4659-b321-b2945a161381\", \"schema.yaml\", 331)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "requirements = \"\"\"pandas\n",
        "pyyaml\n",
        "dask[dataframe]\n",
        "modin[all]\n",
        "ray\n",
        "\"\"\"\n",
        "\n",
        "with open(\"requirements.txt\", \"w\") as f:\n",
        "    f.write(requirements)\n"
      ],
      "metadata": {
        "id": "Vuapupp37Qw2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"requirements.txt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "bfZNInJX7QGk",
        "outputId": "60450a1c-6bba-4c95-cb93-24db348d200e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_450f6601-f54e-4baf-a545-9754591d0a5d\", \"requirements.txt\", 45)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "McOBnopl7P3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jvAmyFos7PgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OmChxGIF64NP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hv13GHlf64QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Attempt to mount Google Drive with authentication\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "BEkSQXm75V5y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}